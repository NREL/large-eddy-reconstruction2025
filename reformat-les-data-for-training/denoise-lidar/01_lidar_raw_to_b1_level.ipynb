{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f3e132-4477-4b5a-b75f-110beb11de4f",
   "metadata": {},
   "source": [
    "Read in synthetic lidar measurements from an AMR-Wind simulation, and reformat the data to look like real-world b-level measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e6d560-a1d6-48cd-9910-247b02d0442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5611f4a3-37b8-4681-a9f7-16003aab7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in data and do some minimal processing\n",
    "parent_dir = Path('/scratch/orybchuk/wakedynamics/bcs-ldm/simulations/072415/large_campaigns/precursor/post_processing')\n",
    "# outdir = Path(parent_dir, 'reformatted')\n",
    "outdir = Path('/scratch/orybchuk/wakedynamics/bcs-ldm/simulations/072415/large_campaigns/precursor/postprocessing', 'reformatted')\n",
    "outdir.mkdir(exist_ok=True,parents=True)\n",
    "t_offset = 43200\n",
    "\n",
    "campaign_files = list(parent_dir.glob('sampling*.nc'))\n",
    "campaign_files.sort()\n",
    "n_campaigns = len(campaign_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4dba7-5833-4a39-9a25-4e6032b12c94",
   "metadata": {},
   "source": [
    "# Common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888bea01-2b32-4e07-b40d-771e40ce9a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varlist: ['velocityx', 'velocityy', 'velocityz']\n"
     ]
    }
   ],
   "source": [
    "### Set up parameters common to all campaigns\n",
    "## User inputs\n",
    "varlist = ['velocityx', 'velocityy', 'velocityz']  # Manually build this\n",
    "use_dask = False\n",
    "\n",
    "## Data from simulations\n",
    "with h5py.File(campaign_files[0]) as f_h5py:\n",
    "    # Deal with time\n",
    "    time = f_h5py['time'][:]\n",
    "    time = np.round(time, 4)  # Round to deal with weird float behavior\n",
    "    timestep = time[1] - time[0]\n",
    "    \n",
    "    # List of variables to process\n",
    "    full_varlist = list(f_h5py['inflow-scan'].keys())\n",
    "for var in varlist:\n",
    "    assert var in full_varlist\n",
    "print(\"varlist:\", varlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cbf765-a237-4601-be7d-bf94d995631b",
   "metadata": {},
   "source": [
    "# Reformat raw AMR-Wind lidar data to look like b-level measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6134db-2789-4fc1-b7c3-df53edb31f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reformatting data 2024-06-16 20:52:15.927492\n",
      "2024-06-16 20:52:15.928657 ... 0\n",
      "Total number of whole reps: 51; Number of extra beam measurements discarded: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 152\u001b[0m\n\u001b[1;32m    150\u001b[0m curr_wspd \u001b[38;5;241m=\u001b[39m ds_trim1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlos_wind_speed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misel(time\u001b[38;5;241m=\u001b[39mtstep)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    151\u001b[0m curr_velx \u001b[38;5;241m=\u001b[39m ds_trim1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvelocityx_true\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misel(time\u001b[38;5;241m=\u001b[39mtstep)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 152\u001b[0m curr_vely \u001b[38;5;241m=\u001b[39m ds_trim1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvelocityy_true\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misel(time\u001b[38;5;241m=\u001b[39mtstep)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    153\u001b[0m curr_velz \u001b[38;5;241m=\u001b[39m ds_trim1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvelocityz_true\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misel(time\u001b[38;5;241m=\u001b[39mtstep)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Identify if this is a 0 microsecond or 500 microsecond moment\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ai4wind/orybchuk/conda/daskenv202404/lib/python3.11/site-packages/xarray/core/dataset.py:1484\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkey)\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mhashable(key):\n\u001b[0;32m-> 1484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_dataarray(key)\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39miterable_of_hashable(key):\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_listed(key)\n",
      "File \u001b[0;32m/projects/ai4wind/orybchuk/conda/daskenv202404/lib/python3.11/site-packages/xarray/core/dataset.py:1405\u001b[0m, in \u001b[0;36mDataset._construct_dataarray\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coord_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables[k]\u001b[38;5;241m.\u001b[39mdims) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m needed_dims:\n\u001b[1;32m   1403\u001b[0m         coords[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables[k]\n\u001b[0;32m-> 1405\u001b[0m indexes \u001b[38;5;241m=\u001b[39m filter_indexes_from_coords(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexes, \u001b[38;5;28mset\u001b[39m(coords))\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataArray(variable, coords, name\u001b[38;5;241m=\u001b[39mname, indexes\u001b[38;5;241m=\u001b[39mindexes, fastpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/projects/ai4wind/orybchuk/conda/daskenv202404/lib/python3.11/site-packages/xarray/core/indexes.py:1498\u001b[0m, in \u001b[0;36mfilter_indexes_from_coords\u001b[0;34m(indexes, filtered_coord_names)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_indexes_from_coords\u001b[39m(\n\u001b[1;32m   1489\u001b[0m     indexes: Mapping[Any, Index],\n\u001b[1;32m   1490\u001b[0m     filtered_coord_names: \u001b[38;5;28mset\u001b[39m,\n\u001b[1;32m   1491\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[Hashable, Index]:\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Filter index items given a (sub)set of coordinate names.\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \n\u001b[1;32m   1494\u001b[0m \u001b[38;5;124;03m    Drop all multi-coordinate related index items for any key missing in the set\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;124;03m    of coordinate names.\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m \n\u001b[1;32m   1497\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1498\u001b[0m     filtered_indexes: \u001b[38;5;28mdict\u001b[39m[Any, Index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(indexes)\n\u001b[1;32m   1500\u001b[0m     index_coord_names: \u001b[38;5;28mdict\u001b[39m[Hashable, \u001b[38;5;28mset\u001b[39m[Hashable]] \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mset\u001b[39m)\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, idx \u001b[38;5;129;01min\u001b[39;00m indexes\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Reformat inflow boundary conditions\n",
    "print(\"Start reformatting data\", datetime.now())\n",
    "\n",
    "for counter, fcampaign in enumerate(campaign_files[:100]):\n",
    "# for counter, fcampaign in enumerate(campaign_files[320:]):\n",
    "    if counter % 10 == 0: print(datetime.now(), '...', counter)\n",
    "    \n",
    "    campaign_id = fcampaign.name[8:12]\n",
    "    \n",
    "    ### ~~~~~~~~~~ Metadata for AMR-Wind inflow lidar ~~~~~~~~~~\n",
    "    with Dataset(fcampaign) as f:\n",
    "        in_time_table = f['inflow-scan'].time_table\n",
    "        in_azimuth_table = f['inflow-scan'].azimuth_table      # NOTE: In practice, these may be shifted from the time_table timestamps\n",
    "        in_elevation_table = f['inflow-scan'].elevation_table  # NOTE: In practice, these may be shifted from the time_table timestamps\n",
    "        in_origin = f['inflow-scan'].start\n",
    "        in_num_points = f['inflow-scan']['points'].shape[1]\n",
    "        in_raw_coordinates = f['inflow-scan']['coordinates'][:].data  # These are just the coords at one moment in time\n",
    "        in_raw_points = f['inflow-scan']['points'][:].data\n",
    "        in_raw_velocityx = f['inflow-scan']['velocityx'][:].data\n",
    "        in_raw_velocityy = f['inflow-scan']['velocityy'][:].data\n",
    "        in_raw_velocityz = f['inflow-scan']['velocityz'][:].data\n",
    "\n",
    "    in_length = np.sqrt((in_raw_coordinates[-1,0]-in_raw_coordinates[0,0])**2 \\\n",
    "                       +(in_raw_coordinates[-1,1]-in_raw_coordinates[0,1])**2 \\\n",
    "                       +(in_raw_coordinates[-1,2]-in_raw_coordinates[0,2])**2)\n",
    "    in_length = np.round(in_length, 2)\n",
    "    in_drange_gate = np.round(in_length/in_num_points, 2)\n",
    "    in_range_gate = np.arange(0, in_length, in_drange_gate)\n",
    "    time_datetime = np.array(pd.to_datetime(time, unit='s'))\n",
    "\n",
    "    in_coords = {'time':time_datetime, 'range_gate': in_range_gate}\n",
    "    in_attrs = {'Range gate length (m)': in_drange_gate,\n",
    "                'Number of gates': float(in_num_points),\n",
    "                'Scan type': 'RAAW inflow',\n",
    "                'Filename': str(fcampaign),\n",
    "                'Origin': in_origin}\n",
    "    \n",
    "    ### ~~~~~~~~~~ Create and populate Xarray Dataset of raw data ~~~~~~~~~~\n",
    "    ds_in = xr.Dataset(coords=in_coords, attrs=in_attrs)\n",
    "\n",
    "    ## Calculate positional data variables\n",
    "    # Helper variables\n",
    "    in_n_full_scans = int(len(time) / len(in_time_table))\n",
    "\n",
    "    in_az_del_x = in_raw_points[:,-1,0] - in_raw_points[:,0,0]\n",
    "    in_az_del_y = in_raw_points[:,-1,1] - in_raw_points[:,0,1]\n",
    "    in_az_del_z = in_raw_points[:,-1,2] - in_origin[2]\n",
    "\n",
    "    # Calculate angles\n",
    "    in_azimuth = (np.rad2deg(np.arctan2(in_az_del_y, in_az_del_x))+360)%360\n",
    "    in_azimuth = np.round(in_azimuth, 2)\n",
    "    for unique_az in np.unique(in_azimuth):\n",
    "        assert unique_az in in_azimuth_table, f\"Azimuth {unique_az} not in the input azimuth table {in_azimuth_table}!\"\n",
    "\n",
    "    in_elevation = (np.rad2deg(np.arcsin(in_az_del_z/in_length))+360)%360\n",
    "    in_elevation = np.round(in_elevation, 2)\n",
    "    for unique_el in np.unique(in_elevation):\n",
    "        assert unique_el in in_elevation_table, f\"Elevation {unique_el} not in the input elevation table {in_elevation_table}!\"\n",
    "\n",
    "    in_pitch = np.zeros_like(in_azimuth)\n",
    "    in_roll = np.zeros_like(in_azimuth)\n",
    "\n",
    "    ## Calculate LOS velocity by projecting (u,v) onto the unit vector aligned with the azimuth\n",
    "    ##  NOTE: By omitting elevation, I think this calc only works at 0 elevation\n",
    "    in_unit_vec_az = np.vstack([np.cos(np.deg2rad(in_azimuth)), np.sin(np.deg2rad(in_azimuth))])\n",
    "    assert np.all(np.isclose(np.sqrt(in_unit_vec_az[0]**2 + in_unit_vec_az[1]**2), 1)), \"Unit vector mangitude doesn't equal 1!\"\n",
    "    in_wind_vec = np.stack((in_raw_velocityx, in_raw_velocityy))\n",
    "    in_los_mag = np.sum(in_wind_vec * in_unit_vec_az[:,:,np.newaxis], axis=0)\n",
    "    in_los_mag = np.abs(in_los_mag)\n",
    "    assert np.abs(np.dot(in_wind_vec[:,-1,-1], in_unit_vec_az[:,-1])) == in_los_mag[-1,-1], \"Error in vectorized dot product calculation!\"\n",
    "\n",
    "    ## Store in Dataset\n",
    "    ds_in['azimuth'] = ('time', in_azimuth)\n",
    "    ds_in['elevation'] = ('time', in_elevation)\n",
    "    ds_in['pitch'] = ('time', in_pitch)\n",
    "    ds_in['roll'] = ('time', in_roll)\n",
    "\n",
    "    ds_in['los_wind_speed'] = (('time', 'range_gate'), in_los_mag)\n",
    "    ds_in['velocityx_true'] = (('time', 'range_gate'), in_raw_velocityx)\n",
    "    ds_in['velocityy_true'] = (('time', 'range_gate'), in_raw_velocityy)\n",
    "    ds_in['velocityz_true'] = (('time', 'range_gate'), in_raw_velocityz)\n",
    "    \n",
    "    ### ~~~~~~~~~~ Match the real-world sampling frequency ~~~~~~~~~~\n",
    "    ### Check some assumptions about the dataset format\n",
    "    # assert ds_in['time'].values[1] - ds_in['time'].values[0] == np.timedelta64(500000000,'ns'), \\\n",
    "    #         \"All the following postprocessing code assumes a 0.5 sec timestep!\"\n",
    "    assert np.timedelta64(1_000_000_000,'ns') % (ds_in['time'].values[1] - ds_in['time'].values[0]) == np.timedelta64(0,'ns'), \\\n",
    "            \"All the following postprocessing code assumes that (1 sec) % (lidar timestep) == 0\"\n",
    "    n_lidar_meas_per_sec = int(np.timedelta64(1_000_000_000,'ns') / (ds_in['time'].values[1] - ds_in['time'].values[0]))\n",
    "    for i in range(1+n_lidar_meas_per_sec):\n",
    "        assert ds_in['azimuth'].values[i] == in_azimuth_table[0], \\\n",
    "            \"All the following code assumes that the AMR-Wind beam position starts at the location of the azimuth table! Add more code to fix this!\"\n",
    "        \n",
    "    ### Drop timestamps where we treat the scan as resetting\n",
    "    skiptimes0 = ds_in['time'].where(ds_in['elevation'] == 45.0)\n",
    "    keeptimes0_flag = (np.isnan(skiptimes0).values).astype(int)\n",
    "    keeptimes0_inds = np.arange(0, len(keeptimes0_flag)) * keeptimes0_flag\n",
    "    keeptimes0_inds = keeptimes0_inds[keeptimes0_flag != 0]\n",
    "\n",
    "    ds_trim0 = ds_in.isel(time=keeptimes0_inds)\n",
    "\n",
    "    ### Toss the first timestamp\n",
    "    assert in_azimuth_table[0] != in_azimuth_table[n_lidar_meas_per_sec], \"The following code is only valid when the 0th timestep is different than the 2nd timestep in the azimuth table\"\n",
    "    assert ds_trim0['azimuth'].values[0] == ds_trim0['azimuth'].values[n_lidar_meas_per_sec], \"If this condition fails, then there's no need to run the below code\"\n",
    "\n",
    "    keeptimes1_inds = np.arange(1, len(ds_trim0['time']))\n",
    "    ds_trim1 = ds_trim0.isel(time=keeptimes1_inds)\n",
    "    \n",
    "    ### Calculate coordinates for the downsampled dataset\n",
    "    # if fcampaign == campaign_files[0]:\n",
    "    #     print(\"WARNING: Manually downsampling to a frequency of 1 second, assuming an AMR-Wind sampling frequency of 0.5 seconds. The following code will not work for other timesteps.\")\n",
    "    assert len(np.unique(pd.DatetimeIndex(ds_trim1['time']).microsecond)) == n_lidar_meas_per_sec, \"Unexpected number of microsecond values! Possible float rounding error?\"\n",
    "    skiptimes2 = ds_trim1['time'].where(pd.DatetimeIndex(ds_trim1['time']).microsecond  != 0)  # Keep timestamps with a microsecond value of 0, \n",
    "                                                                                               # effectively downsampling to 1 sec resolution\n",
    "    keeptimes2_flag = (np.isnan(skiptimes2).values).astype(int)\n",
    "    keeptimes2_inds = np.arange(0, len(keeptimes2_flag)) * keeptimes2_flag\n",
    "    keeptimes2_inds = keeptimes2_inds[keeptimes2_flag != 0]\n",
    "\n",
    "    keeptimes2 = ds_trim1['time'].isel(time=keeptimes2_inds).values\n",
    "    coords2 = {'time': keeptimes2,\n",
    "               'range_gate': ds_trim1['range_gate']}\n",
    "    # keeptimes2\n",
    "\n",
    "    ds_trim2 = xr.Dataset(coords=coords2)\n",
    "    \n",
    "    ### Collect time-averaged and instantaneous winds at our downsampled frequency\n",
    "    az2 = np.zeros(len(ds_trim2['time']))\n",
    "    el2 = np.zeros(len(ds_trim2['time']))\n",
    "    wspd2_avg = np.zeros((len(ds_trim2['time']), \n",
    "                          len(ds_trim2['range_gate'])))\n",
    "    velx2_avg = np.zeros((len(ds_trim2['time']), \n",
    "                          len(ds_trim2['range_gate'])))\n",
    "    vely2_avg = np.zeros((len(ds_trim2['time']), \n",
    "                          len(ds_trim2['range_gate'])))\n",
    "    velz2_avg = np.zeros((len(ds_trim2['time']), \n",
    "                          len(ds_trim2['range_gate'])))\n",
    "    wspd2_inst = np.zeros((len(ds_trim2['time']), \n",
    "                          len(ds_trim2['range_gate'])))\n",
    "    velx2_inst = np.zeros((len(ds_trim2['time']), \n",
    "                          len(ds_trim2['range_gate'])))\n",
    "    vely2_inst = np.zeros((len(ds_trim2['time']), \n",
    "                          len(ds_trim2['range_gate'])))\n",
    "    velz2_inst = np.zeros((len(ds_trim2['time']), \n",
    "                          len(ds_trim2['range_gate'])))\n",
    "\n",
    "    for tstep in range(len(ds_trim1['time'])):\n",
    "        curr_time = ds_trim1['time'].isel(time=tstep).values\n",
    "        curr_az = ds_trim1['azimuth'].isel(time=tstep).values\n",
    "        curr_el = ds_trim1['elevation'].isel(time=tstep).values\n",
    "        curr_wspd = ds_trim1['los_wind_speed'].isel(time=tstep).values\n",
    "        curr_velx = ds_trim1['velocityx_true'].isel(time=tstep).values\n",
    "        curr_vely = ds_trim1['velocityy_true'].isel(time=tstep).values\n",
    "        curr_velz = ds_trim1['velocityz_true'].isel(time=tstep).values\n",
    "\n",
    "        # Identify if this is a 0 microsecond or 500 microsecond moment\n",
    "        first_nonzero_microsecond_val = 1_000_000 / n_lidar_meas_per_sec\n",
    "        if pd.DatetimeIndex([curr_time]).microsecond[0]  == first_nonzero_microsecond_val:  # This is a super ugly if statement, apologies to anyone reading this\n",
    "            # avg_into_time = ds_trim1['time'].isel(time=tstep+1)\n",
    "            # assert (ds_trim1['time'].isel(time=tstep+1) - ds_trim1['time'].isel(time=tstep)).values == np.timedelta64(500000000,'ns'), f\"The next timestep {avg_into_time} should be 0.5 seconds away from the current time {curr_time}!\"\n",
    "            for sub_tstep in range(1,n_lidar_meas_per_sec):\n",
    "                assert ds_trim1['azimuth'].isel(time=tstep+sub_tstep).values == ds_trim1['azimuth'].isel(time=tstep).values, f\"The next azimuth {ds_trim1['azimuth'].isel(time=tstep+sub_tstep).values} should equal the current azimuth {ds_trim1['azimuth'].isel(time=tstep).values}!\"\n",
    "                assert ds_trim1['elevation'].isel(time=tstep+sub_tstep).values == ds_trim1['elevation'].isel(time=tstep).values, f\"The next elevation {ds_trim1['elevation'].isel(time=tstep+sub_tstep).values} should equal the current elevation {ds_trim1['elevation'].isel(time=tstep).values}!\"\n",
    "                \n",
    "            \n",
    "            # Collect wind speeds for averaging\n",
    "            # TODO: The below lines happen because we're hardcoding. Generalize this code.\n",
    "            wspd_avg_list = [curr_wspd]\n",
    "            velx_avg_list = [curr_velx]\n",
    "            vely_avg_list = [curr_vely]\n",
    "            velz_avg_list = [curr_velz]\n",
    "\n",
    "        elif pd.DatetimeIndex([curr_time]).microsecond[0]  != 0:\n",
    "            # Average wind speeds and save out winds\n",
    "            wspd_avg_list.append(curr_wspd)\n",
    "            velx_avg_list.append(curr_velx)\n",
    "            vely_avg_list.append(curr_vely)\n",
    "            velz_avg_list.append(curr_velz)\n",
    "            \n",
    "        else:\n",
    "            # Average wind speeds and save out winds\n",
    "            wspd_avg_list.append(curr_wspd)\n",
    "            velx_avg_list.append(curr_velx)\n",
    "            vely_avg_list.append(curr_vely)\n",
    "            velz_avg_list.append(curr_velz)\n",
    "\n",
    "    #         # Eyeball check that the two timesteps produce similar values\n",
    "    #         print(\"WSPD\")\n",
    "    #         print(wspd_avg_list[0][-5:])\n",
    "    #         print(wspd_avg_list[1][-5:])\n",
    "    #         print()\n",
    "\n",
    "    #         print(\"VELX\")\n",
    "    #         print(velx_avg_list[0][-5:])\n",
    "    #         print(velx_avg_list[1][-5:])\n",
    "    #         print()\n",
    "\n",
    "            # # Do an extra check (this is a poor test for winds centered around 0)\n",
    "            # for currlist in [wspd_avg_list, velx_avg_list, vely_avg_list, velz_avg_list]:\n",
    "            #     assert (((currlist[0] - currlist[1]) / currlist[1]) * 100 < 10).all(), \\\n",
    "            #         \"Difference between measured winds 0.5 sec apart exceeds 10%! That's unexpected\"\n",
    "\n",
    "            outstep = int(tstep / n_lidar_meas_per_sec)\n",
    "\n",
    "            wspd2_avg[outstep, :] = np.mean(wspd_avg_list, axis=0)\n",
    "            velx2_avg[outstep, :] = np.mean(velx_avg_list, axis=0)\n",
    "            vely2_avg[outstep, :] = np.mean(vely_avg_list, axis=0)\n",
    "            velz2_avg[outstep, :] = np.mean(velz_avg_list, axis=0)\n",
    "\n",
    "            # Save out instantaneous winds\n",
    "            wspd2_inst[outstep, :] = curr_wspd\n",
    "            velx2_inst[outstep, :] = curr_velx\n",
    "            vely2_inst[outstep, :] = curr_vely\n",
    "            velz2_inst[outstep, :] = curr_velz        \n",
    "\n",
    "\n",
    "    ## Store data in the Dataset\n",
    "    ds_trim2['los_wspd_avg'] = (('time', 'range_gate'), wspd2_avg)\n",
    "    ds_trim2['velx_true_avg'] = (('time', 'range_gate'), velx2_avg)\n",
    "    ds_trim2['vely_true_avg'] = (('time', 'range_gate'), vely2_avg)\n",
    "    ds_trim2['velz_true_avg'] = (('time', 'range_gate'), velz2_avg)\n",
    "    ds_trim2['los_wspd_inst'] = (('time', 'range_gate'), wspd2_inst)\n",
    "    ds_trim2['velx_true_inst'] = (('time', 'range_gate'), velx2_inst)\n",
    "    ds_trim2['vely_true_inst'] = (('time', 'range_gate'), vely2_inst)\n",
    "    ds_trim2['velz_true_inst'] = (('time', 'range_gate'), velz2_inst)\n",
    "    \n",
    "    ### Collect all non-velocity information for ds_trim2\n",
    "    ds_trim2['azimuth'] = ds_trim1['azimuth'].sel(time=ds_trim2['time'].values)\n",
    "    ds_trim2['elevation'] = ds_trim1['elevation'].sel(time=ds_trim2['time'].values)\n",
    "    ds_trim2['pitch'] = ds_trim1['pitch'].sel(time=ds_trim2['time'].values)\n",
    "    ds_trim2['roll'] = ds_trim1['roll'].sel(time=ds_trim2['time'].values)\n",
    "    \n",
    "    ### Trim off any dangling measurements\n",
    "    n_unique_beams = len(np.unique(np.vstack((ds_trim2['azimuth'].values, ds_trim2['elevation'].values)).T, axis=0))\n",
    "    n_whole_reps = np.floor(len(ds_trim2['time'])/n_unique_beams).astype(int)\n",
    "    n_remainder_reps = len(ds_trim2['time']) % n_unique_beams\n",
    "    if fcampaign == campaign_files[0]:\n",
    "        print(f\"Total number of whole reps: {n_whole_reps}; Number of extra beam measurements discarded: {n_remainder_reps}\")\n",
    "\n",
    "    ds_out = ds_trim2.isel(time=slice(0,n_unique_beams*n_whole_reps))\n",
    "    ds_out.to_netcdf(Path(outdir,f'inflow_b1lev{campaign_id}.nc'))\n",
    "    \n",
    "print(\"End reformatting data\", datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fbf5c-2847-4588-8ee2-8518f00c6a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080b01c-499b-4d13-8ef9-f5fb51620e2a",
   "metadata": {},
   "source": [
    "# Sanity check b-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c002c1-75b1-4cc0-80b8-3915c169b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp0 = xr.open_dataset(Path(outdir,f'inflow_b1lev0000.nc'))\n",
    "tmp1 = xr.open_dataset(Path(outdir,f'inflow_b1lev0001.nc'))\n",
    "tmp2 = xr.open_dataset(Path(outdir,f'inflow_b1lev0002.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbdf20-abb4-458f-b3b6-fa1585c1aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ccb9c-ce3b-4bc6-b335-2f87cc6419a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84193f7-cc79-412a-a06a-ec03ad378fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daskenv202404",
   "language": "python",
   "name": "daskenv202404"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
